#!/usr/bin/env python3
"""
üéØ FASHIONCLIP –ú–û–î–ï–õ–¨ - –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†
"""

import os
import torch
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
import cv2

os.environ['HF_HOME'] = os.path.expanduser('~/.cache/huggingface')
os.environ['TRANSFORMERS_CACHE'] = os.path.expanduser('~/.cache/huggingface')

from transformers import CLIPModel, CLIPProcessor

class FashionCLIPAnalyzer:
    def __init__(self):
        """FashionCLIP –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –æ–¥–µ–∂–¥—ã"""
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FashionCLIP...")
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º FashionCLIP
        self.model = CLIPModel.from_pretrained('patrickjohncyh/fashion-clip').to(self.device)
        self.processor = CLIPProcessor.from_pretrained('patrickjohncyh/fashion-clip')
        
        # –ë–∞–∑–∞ –ø—Ä–æ–º—Ç–æ–≤
        self.categories = self._create_fashion_prompts()
        print("‚úÖ FashionCLIP –∑–∞–≥—Ä—É–∂–µ–Ω!")
    
    def _create_fashion_prompts(self):
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º—Ç—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        return {
            'garment_category': [
                "winter coat", "trench coat", "peacoat", "parka jacket", "puffer jacket",
                "bomber jacket", "leather jacket", "denim jacket", "blazer", "cardigan",
                "hoodie", "sweater", "dress shirt", "casual shirt", "flannel shirt",
                "polo shirt", "t-shirt", "blouse", "silk blouse", "cocktail dress",
                "maxi dress", "mini dress", "a-line skirt", "pencil skirt", "dress pants",
                "chino pants", "cargo pants", "skinny jeans", "straight jeans", "leggings",
                "shorts", "swimsuit", "bikini", "kimono", "sari"
            ],
            'material_fabric': [
                "cotton fabric", "linen fabric", "silk fabric", "wool fabric", "denim fabric",
                "leather material", "suede material", "polyester fabric", "nylon fabric",
                "velvet fabric", "satin fabric", "chiffon fabric"
            ],
            'color_pattern': [
                "black color", "white color", "navy blue", "gray", "beige color",
                "brown color", "burgundy color", "green color", "purple color", "pink color",
                "red color", "blue color", "striped pattern", "floral print", "animal print",
                "geometric pattern", "solid color"
            ],
            'style_occasion': [
                "casual style", "formal style", "business style", "sporty style", "vintage style",
                "bohemian style", "streetwear style", "evening occasion", "office appropriate"
            ]
        }
    
    def analyze_image(self, image: Image.Image) -> dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ–¥–µ–∂–¥—ã"""
        try:
            analysis = {}
            
            for category, options in self.categories.items():
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –≤—Ö–æ–¥—ã
                text_inputs = self.processor(text=options, return_tensors="pt", padding=True)
                text_inputs = {k: v.to(self.device) for k, v in text_inputs.items()}
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_inputs = self.processor(images=image, return_tensors="pt")
                image_inputs = {k: v.to(self.device) for k, v in image_inputs.items()}
                
                with torch.no_grad():
                    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                    image_features = self.model.get_image_features(**image_inputs)
                    text_features = self.model.get_text_features(**text_inputs)
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                    image_features = torch.nn.functional.normalize(image_features, p=2, dim=1)
                    text_features = torch.nn.functional.normalize(text_features, p=2, dim=1)
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
                    similarities = torch.matmul(image_features, text_features.T)[0]
                    similarities = similarities / 2.0  # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–∞—è —à–∫–∞–ª–∞
                    probabilities = torch.softmax(similarities, dim=0)
                
                # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                top_indices = torch.topk(probabilities, 3).indices
                top_results = [
                    {'item': options[i], 'confidence': probabilities[i].item()}
                    for i in top_indices
                ]
                
                analysis[category] = {
                    'best_match': top_results[0],
                    'alternatives': top_results[1:]
                }
            
            return analysis
            
        except Exception as e:
            return {"error": f"FashionCLIP –æ—à–∏–±–∫–∞: {str(e)}"}